{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7635c0a2-17a4-4f61-b2d0-8aebb6ce196b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "import random\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"policy_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"policy_type\", StringType(), True),\n",
    "    StructField(\"premium_amount\", DoubleType(), True),\n",
    "    StructField(\"start_date\", StringType(), True),\n",
    "    StructField(\"end_date\", StringType(), True),\n",
    "    StructField(\"claim_count\", IntegerType(), True),\n",
    "    StructField(\"last_claim_date\", StringType(), True),\n",
    "    StructField(\"agent_id\", IntegerType(), True),\n",
    "    StructField(\"region\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Sample insurance data\n",
    "data = [\n",
    "    (1001, 501, \"Alice Smith\", \"Health\", 1200.50, \"2023-01-01\", \"2024-01-01\", 2, \"2023-11-15\", 301, \"East\"),\n",
    "    (1002, 502, \"Bob Johnson\", \"Auto\", 800.00, \"2022-06-15\", \"2023-06-15\", 1, \"2023-05-20\", 302, \"West\"),\n",
    "    (1003, 503, \"Carol Lee\", \"Home\", 950.75, \"2023-03-10\", \"2024-03-10\", 0, None, 303, \"North\"),\n",
    "    (1004, 504, \"David Kim\", \"Life\", 1500.00, \"2021-09-01\", \"2026-09-01\", 3, \"2024-02-10\", 304, \"South\"),\n",
    "    (1005, 505, \"Eva Brown\", \"Health\", 1100.25, \"2022-12-01\", \"2023-12-01\", 1, \"2023-08-05\", 305, \"East\"),\n",
    "    (1006, 506, \"Frank White\", \"Auto\", 700.00, \"2023-05-20\", \"2024-05-20\", 0, None, 306, \"West\"),\n",
    "    (1007, 507, \"Grace Green\", \"Home\", 980.00, \"2022-07-15\", \"2023-07-15\", 2, \"2023-06-30\", 307, \"North\"),\n",
    "    (1008, 508, \"Henry Black\", \"Life\", 1600.00, \"2020-10-01\", \"2025-10-01\", 4, \"2024-03-12\", 308, \"South\"),\n",
    "    (1009, 509, \"Ivy King\", \"Health\", 1250.00, \"2023-02-01\", \"2024-02-01\", 1, \"2023-09-18\", 309, \"East\"),\n",
    "    (1010, 510, \"Jack Young\", \"Auto\", 850.00, \"2022-11-10\", \"2023-11-10\", 2, \"2023-10-25\", 310, \"West\")\n",
    "]\n",
    "\n",
    "# Add 100 more rows\n",
    "policy_types = [\"Health\", \"Auto\", \"Home\", \"Life\"]\n",
    "regions = [\"East\", \"West\", \"North\", \"South\"]\n",
    "names = [\"Alex\", \"Jordan\", \"Taylor\", \"Morgan\", \"Casey\", \"Riley\", \"Jamie\", \"Drew\", \"Robin\", \"Skyler\"]\n",
    "\n",
    "for i in range(1011, 1111):\n",
    "    customer_id = 500 + i\n",
    "    customer_name = f\"{random.choice(names)} {random.choice(['Smith', 'Johnson', 'Lee', 'Kim', 'Brown', 'White', 'Green', 'Black', 'King', 'Young'])}\"\n",
    "    policy_type = random.choice(policy_types)\n",
    "    premium_amount = round(random.uniform(700, 1700), 2)\n",
    "    start_date = f\"2023-{random.randint(1,12):02d}-{random.randint(1,28):02d}\"\n",
    "    end_date = f\"2024-{random.randint(1,12):02d}-{random.randint(1,28):02d}\"\n",
    "    claim_count = random.randint(0, 4)\n",
    "    last_claim_date = None if claim_count == 0 else f\"2023-{random.randint(1,12):02d}-{random.randint(1,28):02d}\"\n",
    "    agent_id = 300 + random.randint(1, 20)\n",
    "    region = random.choice(regions)\n",
    "    data.append((i, customer_id, customer_name, policy_type, premium_amount, start_date, end_date, claim_count, last_claim_date, agent_id, region))\n",
    "\n",
    "# Create DataFrame\n",
    "insurance_bronze_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Save as bronze table\n",
    "insurance_bronze_df.write.mode(\"overwrite\").saveAsTable(\"insurance_bronze\")\n",
    "\n",
    "print(\"Bronze layer 'insurance_bronze' table created successfully\")\n",
    "\n",
    "display(spark.table(\"insurance_bronze\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7d2c762-3bb5-494f-907c-317462f3f88c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim\n",
    "\n",
    "# Read from bronze table\n",
    "bronze_df = spark.table(\"insurance_bronze\")\n",
    "\n",
    "# Remove rows with any null values\n",
    "clean_df = bronze_df.dropna(how=\"any\")\n",
    "\n",
    "# Remove duplicate rows\n",
    "clean_df = clean_df.dropDuplicates()\n",
    "\n",
    "# Cast all date columns to string and trim whitespaces in string columns\n",
    "columns_to_trim = [\"customer_name\", \"policy_type\", \"region\"]\n",
    "for c in columns_to_trim:\n",
    "    clean_df = clean_df.withColumn(c, trim(col(c)))\n",
    "\n",
    "date_columns = [\"start_date\", \"end_date\", \"last_claim_date\"]\n",
    "for d in date_columns:\n",
    "    clean_df = clean_df.withColumn(d, col(d).cast(\"string\"))\n",
    "\n",
    "# Save as silver table\n",
    "clean_df.write.mode(\"overwrite\").saveAsTable(\"insurance_silver\")\n",
    "\n",
    "print(\"Silver layer 'insurance_silver' table created successfully\")\n",
    "\n",
    "display(spark.table(\"insurance_silver\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f66dd9d3-5319-41f9-8cdf-cc95ab1ee0de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct, sum, min, max\n",
    "\n",
    "# Read from silver table\n",
    "silver_df = spark.table(\"insurance_silver\")\n",
    "\n",
    "# Aggregate: count of distinct customers per policy_type\n",
    "policy_type_df = silver_df.groupBy(\"policy_type\").agg(\n",
    "    countDistinct(\"customer_id\").alias(\"customer_count\")\n",
    ")\n",
    "\n",
    "# Aggregate: total premium, min/max start_date per customer\n",
    "customer_df = silver_df.groupBy(\"customer_id\").agg(\n",
    "    sum(\"premium_amount\").alias(\"total_premium\"),\n",
    "    min(\"start_date\").alias(\"min_start_date\"),\n",
    "    max(\"end_date\").alias(\"max_end_date\")\n",
    ")\n",
    "\n",
    "# Save as gold tables\n",
    "policy_type_df.write.mode(\"overwrite\").saveAsTable(\"insurance_gold_policy_type\")\n",
    "customer_df.write.mode(\"overwrite\").saveAsTable(\"insurance_gold_customer\")\n",
    "\n",
    "print(\"Gold layer 'insurance_gold_policy_type' and 'insurance_gold_customer' tables created successfully\")\n",
    "\n",
    "display(spark.table(\"insurance_gold_policy_type\"))\n",
    "display(spark.table(\"insurance_gold_customer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce59efa7-f314-465f-90cd-a12aecd5277a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE Employee (\n",
    "    EmpID INT PRIMARY KEY,\n",
    "    Name VARCHAR(50) NOT NULL,\n",
    "    Department VARCHAR(50) NOT NULL,\n",
    "    Salary DECIMAL(10,2) NOT NULL,\n",
    "    JoinDate DATE NOT NULL\n",
    ");\n",
    "\n",
    "-- Insert sample data\n",
    "INSERT INTO Employee (EmpID, Name, Department, Salary, JoinDate) VALUES\n",
    "(101, 'Alice',   'HR',      50000, '2020-01-15'),\n",
    "(102, 'Bob',     'HR',      60000, '2019-03-10'),\n",
    "(103, 'Charlie', 'IT',      70000, '2021-07-01'),\n",
    "(104, 'David',   'IT',      80000, '2018-11-20'),\n",
    "(105, 'Emma',    'Finance', 75000, '2022-02-05'),\n",
    "(106, 'Frank',   'Finance', 65000, '2020-06-12'),\n",
    "(107, 'Grace',   'IT',      90000, '2017-09-25');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b4c27b6-f8c1-4f85-aff5-d4702531e7e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select department,max(salary) from employee group by department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf8b3bbb-a7cc-4ba0-967c-762695c85ac6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "want to find max salary from each department in each month"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select max(salary),department, month(joinDate) from employee group by department,month(joinDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42a58c6f-316a-4305-835d-aed152ecbf14",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "want to fetch employee details for max(salary) without using window function"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select e.empid,e.name,e.department,e.Salary from employee e join (select department,max(salary) as m_sal from employee group by department) d on e.Department=d.department and e.salary=d.m_sal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d20a6b34-99b7-4e40-9628-02c40cd0b562",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "using widow function"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "  EmpID, Name, Department, Salary,\n",
    "  RANK() OVER (PARTITION BY Department ORDER BY Salary DESC) AS SalaryRank\n",
    "FROM Employee;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4679953375372133,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Gen AI DB Training",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
